{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LIME Explanation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ste93na/XAI-Public/blob/master/LIME_Explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtR7cyFs5W9D",
        "colab_type": "text"
      },
      "source": [
        "This project is about explaining what machine learning classifiers (or models) are doing. At fisrt  only explaining individual predictions for text classifiers was supported, then there was an improovement  for classifiers that act on tables (numpy arrays of numerical or categorical data) or images.\n",
        "LIME stands for:\n",
        "\n",
        "\n",
        "*   Local : Lime approximate the black-box model locally, using the neighborhood of the prediction beeing explained\n",
        "*   Interpretable : Explanation produced by LIME is simple to undestrand for humans\n",
        "*   Model-Agnostic : Lime treats models as black-box\n",
        "*   Explanations : provide explanation of the model\n",
        "\n",
        "\n",
        "To explain an instance, LIME samples other instances and weighs them by the proximity\n",
        "to the instance being explained.The use a linear model to learn the explanation\n",
        "that is locally (but not globally) faithful. Moreover Lime is able to explain any black box classifier, with two or more classes.\n",
        "\n",
        "For more information read https://github.com/marcotcr/lime (contains also pdf of the paper).\n",
        "Now we'll explain LIME code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUxk-7NW9LwE",
        "colab_type": "text"
      },
      "source": [
        "**lime-base.py**\n",
        "\n",
        "This library contains all the base function used by all other Classes to do the explanation. \n",
        "\n",
        "LimeBase is the Class for learning a locally linear sparse model from perturbed data that are drawn sample near the istance to classify.\n",
        "\n",
        "*To better undestand what perturbed data stands for:\n",
        "From M. Ribeiro LIME paper ' We sample instances around x0 by drawing nonzero\n",
        "elements of x0 uniformly at random'* (Look at the chapter 3 of the paper to learn the theory behind this code)\n",
        "\n",
        "Distances between the instance and other data are used to calculate weights calling the kernel function that transforms an array of distances into an array of proximity values, because LIME weighs instance  by the proximity to the instance being explained. This kernel function (one of the information used to inizialize the class) is called in *explain_instance_with_data* function.\n",
        "\n",
        "Than this function calls features selection procedure that take as input parameters the perturbed data and their corresponding perturbed labels, weights (computed previously), maximum number of features in explanation and method, that tell how to select features. Possible methods are forward selection or K-lasso (see the coment added to the code below for understand how they behave). The K Features, representing the lenght of the explanation, are chosen according to a specific metric, like a score calculating iteratively adding each feature once at time to the fitting of the model (forward selection), or computing weights in other ways (using coefficients with K-lasso or computing weights with highest_weights method).\n",
        "\n",
        "Finally LimeBase do the explanation. Lime use a linear model to explain predictions locally, that's why is used a linear regressor. In particular if no regressor is specified, is used the ridge regressor as default.  The model is fitted with perturbed data to return: intercept, exp that is a sorted list of tuples, where each tuple (x,y) corresponds to the feature id (x) and the local weight (y), score that is the R^2 value of the returned explanation (to estimate the goodness of the explanation),  local_pred that is the prediction of the explanation model on the original instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA3GqGTm0nlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Contains abstract functionality for learning locally linear sparse model.\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.linear_model import Ridge, lars_path\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "\n",
        "class LimeBase(object):\n",
        "    \"\"\"Class for learning a locally linear sparse model from perturbed data\"\"\"\n",
        "    def __init__(self,\n",
        "                 kernel_fn,\n",
        "                 verbose=False,\n",
        "                 random_state=None):\n",
        "        \"\"\"Init function\n",
        "        Args:\n",
        "            kernel_fn: function that transforms an array of distances into an\n",
        "                        array of proximity values (floats).\n",
        "            verbose: if true, print local prediction values from linear model.\n",
        "            random_state: an integer or numpy.RandomState that will be used to\n",
        "                generate random numbers. If None, the random state will be\n",
        "                initialized using the internal numpy seed.\n",
        "        \"\"\"\n",
        "        self.kernel_fn = kernel_fn\n",
        "        self.verbose = verbose\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_lars_path(weighted_data, weighted_labels):\n",
        "        \"\"\"Generates the lars path for weighted data. Computes Lasso Path (for feature selection) along the regularization parameter using the LARS algorithm.\n",
        "       Indeed LIME approximate the complexity measure by first selecting K features with Lasso and then learning the weights via least squares (a\n",
        "      procedure called K-LASSO) Check chapter 3 of the paper for more informations.\n",
        "        Args:\n",
        "            weighted_data: data that has been weighted by kernel\n",
        "            weighted_label: labels, weighted by kernel\n",
        "        Returns:\n",
        "            (alphas, coefs), both are arrays corresponding to the\n",
        "            regularization parameter and coefficients, respectively\n",
        "        \"\"\"\n",
        "        x_vector = weighted_data\n",
        "        alphas, _, coefs = lars_path(x_vector,\n",
        "                                     weighted_labels,\n",
        "                                     method='lasso',\n",
        "                                     verbose=False)\n",
        "        return alphas, coefs\n",
        "\n",
        "    def forward_selection(self, data, labels, weights, num_features):\n",
        "        \"\"\"Iteratively adds features to the model.\n",
        "        Do the procedure n time, where n is the minimum between the user parameter num_feature and the number of the features of the dataset ( data.shape[1]).\n",
        "        In each iteration, for each feature of the dataset calculate the score fitting the linear model(ridge) with the features added so far. If the score is better \n",
        "        then previous one, and the feature has not been added yet, add the feature to the model\"\"\"\n",
        "        clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n",
        "        used_features = []\n",
        "        for _ in range(min(num_features, data.shape[1])):\n",
        "            max_ = -100000000\n",
        "            best = 0\n",
        "            for feature in range(data.shape[1]):\n",
        "                if feature in used_features:\n",
        "                    continue\n",
        "                clf.fit(data[:, used_features + [feature]], labels,\n",
        "                        sample_weight=weights)\n",
        "                score = clf.score(data[:, used_features + [feature]],\n",
        "                                  labels,\n",
        "                                  sample_weight=weights)\n",
        "                if score > max_:\n",
        "                    best = feature\n",
        "                    max_ = score\n",
        "            used_features.append(best)\n",
        "        return np.array(used_features)\n",
        "\n",
        "    def feature_selection(self, data, labels, weights, num_features, method):\n",
        "        \"\"\"Selects features for the model. see explain_instance_with_data to understand the parameters.\n",
        "        Parameters:\n",
        "          data - perturbed data passed by explain_instance_with_data\n",
        "          labels - corresponding perturbed labels\n",
        "          weights - are calculated as distances to original data point\n",
        "          num_features - maximum number of features in explanation\n",
        "          method - tell how to select the features. Options are described in explain_instance_with_data\n",
        "        Function:  \n",
        "           If there is no method specified, selects all the features\n",
        "           If the method is forward selection, call the respective previous function\n",
        "           If the method is highest weights then fit the model to calculate coefficients. If the dataset is sparse, create a sparse matrix and multiply data \n",
        "           with coefficients to generate weighted data, else multiply coefficients with all the data to generate weighted data. \n",
        "           Finally Return features with highest weights\n",
        "           If the method is lasso_path then the algorithm calculate weighted data via least squares, then call generate_lars_path passing the weighted data,\n",
        "           to calculate the regularization coefficients, and finally select the features(K-LASSO).\n",
        "           If the method is auto the if the number of the features is less then 6, the algorithm uses the forward selection, else is used the highest wheights \n",
        "           method \"\"\"\n",
        "        \n",
        "        if method == 'none':\n",
        "            return np.array(range(data.shape[1]))\n",
        "        elif method == 'forward_selection':\n",
        "            return self.forward_selection(data, labels, weights, num_features)\n",
        "        elif method == 'highest_weights':\n",
        "            clf = Ridge(alpha=0, fit_intercept=True,\n",
        "                        random_state=self.random_state)\n",
        "            clf.fit(data, labels, sample_weight=weights)\n",
        "\n",
        "            coef = clf.coef_\n",
        "            if sp.sparse.issparse(data):\n",
        "                coef = sp.sparse.csr_matrix(clf.coef_)\n",
        "                weighted_data = coef.multiply(data[0])\n",
        "                # Note: most efficient to slice the data before reversing\n",
        "                sdata = len(weighted_data.data)\n",
        "                argsort_data = np.abs(weighted_data.data).argsort()\n",
        "                # Edge case where data is more sparse than requested number of feature importances\n",
        "                # In that case, we just pad with zero-valued features\n",
        "                if sdata < num_features:\n",
        "                    nnz_indexes = argsort_data[::-1]\n",
        "                    indices = weighted_data.indices[nnz_indexes]\n",
        "                    num_to_pad = num_features - sdata\n",
        "                    indices = np.concatenate((indices, np.zeros(num_to_pad, dtype=indices.dtype)))\n",
        "                    indices_set = set(indices)\n",
        "                    pad_counter = 0\n",
        "                    for i in range(data.shape[1]):\n",
        "                        if i not in indices_set:\n",
        "                            indices[pad_counter + sdata] = i\n",
        "                            pad_counter += 1\n",
        "                            if pad_counter >= num_to_pad:\n",
        "                                break\n",
        "                else:\n",
        "                    nnz_indexes = argsort_data[sdata - num_features:sdata][::-1]\n",
        "                    indices = weighted_data.indices[nnz_indexes]\n",
        "                return indices\n",
        "            else:\n",
        "                weighted_data = coef * data[0]\n",
        "                feature_weights = sorted(\n",
        "                    zip(range(data.shape[1]), weighted_data),\n",
        "                    key=lambda x: np.abs(x[1]),\n",
        "                    reverse=True)\n",
        "                return np.array([x[0] for x in feature_weights[:num_features]])\n",
        "        elif method == 'lasso_path':\n",
        "            weighted_data = ((data - np.average(data, axis=0, weights=weights))\n",
        "                             * np.sqrt(weights[:, np.newaxis]))\n",
        "            weighted_labels = ((labels - np.average(labels, weights=weights))\n",
        "                               * np.sqrt(weights))\n",
        "            nonzero = range(weighted_data.shape[1])\n",
        "            _, coefs = self.generate_lars_path(weighted_data,\n",
        "                                               weighted_labels)\n",
        "            for i in range(len(coefs.T) - 1, 0, -1):\n",
        "                nonzero = coefs.T[i].nonzero()[0]\n",
        "                if len(nonzero) <= num_features:\n",
        "                    break\n",
        "            used_features = nonzero\n",
        "            return used_features\n",
        "        elif method == 'auto':\n",
        "            if num_features <= 6:\n",
        "                n_method = 'forward_selection'\n",
        "            else:\n",
        "                n_method = 'highest_weights'\n",
        "            return self.feature_selection(data, labels, weights,\n",
        "                                          num_features, n_method)\n",
        "\n",
        "    def explain_instance_with_data(self,\n",
        "                                   neighborhood_data,\n",
        "                                   neighborhood_labels,\n",
        "                                   distances,\n",
        "                                   label,\n",
        "                                   num_features,\n",
        "                                   feature_selection='auto',\n",
        "                                   model_regressor=None):\n",
        "        \"\"\"Takes perturbed data, labels and distances, returns explanation.\n",
        "        Args:\n",
        "            neighborhood_data: perturbed data, 2d array. first element is\n",
        "                               assumed to be the original data point.\n",
        "            neighborhood_labels: corresponding perturbed labels. should have as\n",
        "                                 many columns as the number of possible labels.\n",
        "            distances: distances to original data point.\n",
        "            label: label for which we want an explanation\n",
        "            num_features: maximum number of features in explanation\n",
        "            feature_selection: how to select num_features. options are:\n",
        "                'forward_selection': iteratively add features to the model.\n",
        "                    This is costly when num_features is high\n",
        "                'highest_weights': selects the features that have the highest\n",
        "                    product of absolute weight * original data point when\n",
        "                    learning with all the features\n",
        "                'lasso_path': chooses features based on the lasso\n",
        "                    regularization path\n",
        "                'none': uses all features, ignores num_features\n",
        "                'auto': uses forward_selection if num_features <= 6, and\n",
        "                    'highest_weights' otherwise.\n",
        "            model_regressor: sklearn regressor to use in explanation.\n",
        "                Defaults to Ridge regression if None. Must have\n",
        "                model_regressor.coef_ and 'sample_weight' as a parameter\n",
        "                to model_regressor.fit()\n",
        "        Returns:\n",
        "            (intercept, exp, score, local_pred):\n",
        "            intercept is a float.\n",
        "            exp is a sorted list of tuples, where each tuple (x,y) corresponds\n",
        "            to the feature id (x) and the local weight (y). The list is sorted\n",
        "            by decreasing absolute value of y.\n",
        "            score is the R^2 value of the returned explanation\n",
        "            local_pred is the prediction of the explanation model on the original instance\n",
        "            \n",
        "        Function:\n",
        "          for weights call the kernel function that transforms an array of distances into an array of proximity values, because LIME weighs instance \n",
        "          by the proximity to the instance being explained. Then is called the feature selection function.\n",
        "          Lime use a linear model to explain predictions locally, that's why is used a linear regressor. In particular if no regressor is specified, is used the\n",
        "          ridge regressor as default model.\n",
        "                      \n",
        "        \"\"\"\n",
        "\n",
        "        weights = self.kernel_fn(distances)\n",
        "        labels_column = neighborhood_labels[:, label]\n",
        "        used_features = self.feature_selection(neighborhood_data,\n",
        "                                               labels_column,\n",
        "                                               weights,\n",
        "                                               num_features,\n",
        "                                               feature_selection)\n",
        "        if model_regressor is None:\n",
        "            model_regressor = Ridge(alpha=1, fit_intercept=True,\n",
        "                                    random_state=self.random_state)\n",
        "        easy_model = model_regressor\n",
        "        easy_model.fit(neighborhood_data[:, used_features],\n",
        "                       labels_column, sample_weight=weights)\n",
        "        prediction_score = easy_model.score(\n",
        "            neighborhood_data[:, used_features],\n",
        "            labels_column, sample_weight=weights)\n",
        "\n",
        "        local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n",
        "\n",
        "        if self.verbose:\n",
        "            print('Intercept', easy_model.intercept_)\n",
        "            print('Prediction_local', local_pred,)\n",
        "            print('Right:', neighborhood_labels[0, label])\n",
        "        return (easy_model.intercept_,\n",
        "                sorted(zip(used_features, easy_model.coef_),\n",
        "                       key=lambda x: np.abs(x[1]), reverse=True),\n",
        "                prediction_score, local_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRwps_zC2vzd",
        "colab_type": "text"
      },
      "source": [
        "**explanations.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZbMhqADZEXf",
        "colab_type": "text"
      },
      "source": [
        "explanations.py contains the Explanation class (the object returned by explainers) and some visualizations function to show the ouput to the user. \n",
        "Contains also a class mapper for mapping features to the specific domain, because there would be a subclass for each domain (text, tables,images, etc), so that we can have a general Explanation class, and separate out the specifics of visualizing features in here.\n",
        "\n",
        "The Explaination class supports both regression and classification and, for the latter, requires the class names.\n",
        "\n",
        "Finally in this code are described some visualizations techniques to show the explainer output to the user. In particular the explanation can be shown as : \n",
        "\n",
        "\n",
        "*   list - in which you have to specify labels that you want to see. If you ask for a label for which an explanation wasn't computed, will throw an exception. Will be ignored for regression explanations\n",
        "*  pyplot_figure - Returns the explanation as a pyplot figure. Will throw an error if you don't have matplotlib \n",
        "* html - Returns the explanation as an html page. You have to set labels to show, prediction probabilities (only for classification) or predicted value (only for regression)  to show a barchart. \n",
        "\n",
        "Moreover the html explanation can be Shown in ipython notebook (IPython required) or saved to file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq9Zf8Be21yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals\n",
        "from io import open\n",
        "import os\n",
        "import os.path\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from .exceptions import LimeError\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "\n",
        "def id_generator(size=15, random_state=None):\n",
        "    \"\"\"Helper function to generate random div ids. This is useful for embedding\n",
        "    HTML into ipython notebooks.\"\"\"\n",
        "    chars = list(string.ascii_uppercase + string.digits)\n",
        "    return ''.join(random_state.choice(chars, size, replace=True))\n",
        "\n",
        "\n",
        "class DomainMapper(object):\n",
        "    \"\"\"Class for mapping features to the specific domain.\n",
        "    The idea is that there would be a subclass for each domain (text, tables,\n",
        "    images, etc), so that we can have a general Explanation class, and separate\n",
        "    out the specifics of visualizing features in here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def map_exp_ids(self, exp, **kwargs):\n",
        "        \"\"\"Maps the feature ids to concrete names.\n",
        "        Default behaviour is the identity function. Subclasses can implement\n",
        "        this as they see fit.\n",
        "        Args:\n",
        "            exp: list of tuples [(id, weight), (id,weight)]\n",
        "            kwargs: optional keyword arguments\n",
        "        Returns:\n",
        "            exp: list of tuples [(name, weight), (name, weight)...]\n",
        "        \"\"\"\n",
        "        return exp\n",
        "\n",
        "    def visualize_instance_html(self,\n",
        "                                exp,\n",
        "                                label,\n",
        "                                div_name,\n",
        "                                exp_object_name,\n",
        "                                **kwargs):\n",
        "        \"\"\"Produces html for visualizing the instance.\n",
        "        Default behaviour does nothing. Subclasses can implement this as they\n",
        "        see fit.\n",
        "        Args:\n",
        "             exp: list of tuples [(id, weight), (id,weight)]\n",
        "             label: label id (integer)\n",
        "             div_name: name of div object to be used for rendering(in js)\n",
        "             exp_object_name: name of js explanation object\n",
        "             kwargs: optional keyword arguments\n",
        "        Returns:\n",
        "             js code for visualizing the instance\n",
        "        \"\"\"\n",
        "        return ''\n",
        "\n",
        "\n",
        "class Explanation(object):\n",
        "    \"\"\"Object returned by explainers.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 domain_mapper,\n",
        "                 mode='classification',\n",
        "                 class_names=None,\n",
        "                 random_state=None):\n",
        "        \"\"\"\n",
        "        Initializer.\n",
        "        Args:\n",
        "            domain_mapper: must inherit from DomainMapper class\n",
        "            type: \"classification\" or \"regression\"\n",
        "            class_names: list of class names (only used for classification)\n",
        "            random_state: an integer or numpy.RandomState that will be used to\n",
        "                generate random numbers. If None, the random state will be\n",
        "                initialized using the internal numpy seed.\n",
        "        \"\"\"\n",
        "        self.random_state = random_state\n",
        "        self.mode = mode\n",
        "        self.domain_mapper = domain_mapper\n",
        "        self.local_exp = {}\n",
        "        self.intercept = {}\n",
        "        self.score = None\n",
        "        self.local_pred = None\n",
        "        self.scaled_data = None\n",
        "        if mode == 'classification':\n",
        "            self.class_names = class_names\n",
        "            self.top_labels = None\n",
        "            self.predict_proba = None\n",
        "        elif mode == 'regression':\n",
        "            self.class_names = ['negative', 'positive']\n",
        "            self.predicted_value = None\n",
        "            self.min_value = 0.0\n",
        "            self.max_value = 1.0\n",
        "            self.dummy_label = 1\n",
        "        else:\n",
        "            raise LimeError('Invalid explanation mode \"{}\". '\n",
        "                            'Should be either \"classification\" '\n",
        "                            'or \"regression\".'.format(mode))\n",
        "\n",
        "    def available_labels(self):\n",
        "        \"\"\"\n",
        "        Returns the list of classification labels for which we have any explanations.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            assert self.mode == \"classification\"\n",
        "        except AssertionError:\n",
        "            raise NotImplementedError('Not supported for regression explanations.')\n",
        "        else:\n",
        "            ans = self.top_labels if self.top_labels else self.local_exp.keys()\n",
        "            return list(ans)\n",
        "\n",
        "    def as_list(self, label=1, **kwargs):\n",
        "        \"\"\"Returns the explanation as a list.\n",
        "        Args:\n",
        "            label: desired label. If you ask for a label for which an\n",
        "                explanation wasn't computed, will throw an exception.\n",
        "                Will be ignored for regression explanations.\n",
        "            kwargs: keyword arguments, passed to domain_mapper\n",
        "        Returns:\n",
        "            list of tuples (representation, weight), where representation is\n",
        "            given by domain_mapper. Weight is a float.\n",
        "        \"\"\"\n",
        "        label_to_use = label if self.mode == \"classification\" else self.dummy_label\n",
        "        ans = self.domain_mapper.map_exp_ids(self.local_exp[label_to_use], **kwargs)\n",
        "        ans = [(x[0], float(x[1])) for x in ans]\n",
        "        return ans\n",
        "\n",
        "    def as_map(self):\n",
        "        \"\"\"Returns the map of explanations.\n",
        "        Returns:\n",
        "            Map from label to list of tuples (feature_id, weight).\n",
        "        \"\"\"\n",
        "        return self.local_exp\n",
        "\n",
        "    def as_pyplot_figure(self, label=1, **kwargs):\n",
        "        \"\"\"Returns the explanation as a pyplot figure.\n",
        "        Will throw an error if you don't have matplotlib installed\n",
        "        Args:\n",
        "            label: desired label. If you ask for a label for which an\n",
        "                   explanation wasn't computed, will throw an exception.\n",
        "                   Will be ignored for regression explanations.\n",
        "            kwargs: keyword arguments, passed to domain_mapper\n",
        "        Returns:\n",
        "            pyplot figure (barchart).\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        exp = self.as_list(label=label, **kwargs)\n",
        "        fig = plt.figure()\n",
        "        vals = [x[1] for x in exp]\n",
        "        names = [x[0] for x in exp]\n",
        "        vals.reverse()\n",
        "        names.reverse()\n",
        "        colors = ['green' if x > 0 else 'red' for x in vals]\n",
        "        pos = np.arange(len(exp)) + .5\n",
        "        plt.barh(pos, vals, align='center', color=colors)\n",
        "        plt.yticks(pos, names)\n",
        "        if self.mode == \"classification\":\n",
        "            title = 'Local explanation for class %s' % self.class_names[label]\n",
        "        else:\n",
        "            title = 'Local explanation'\n",
        "        plt.title(title)\n",
        "        return fig\n",
        "\n",
        "    def show_in_notebook(self,\n",
        "                         labels=None,\n",
        "                         predict_proba=True,\n",
        "                         show_predicted_value=True,\n",
        "                         **kwargs):\n",
        "        \"\"\"Shows html explanation in ipython notebook.\n",
        "        See as_html() for parameters.\n",
        "        This will throw an error if you don't have IPython installed\"\"\"\n",
        "\n",
        "        from IPython.core.display import display, HTML\n",
        "        display(HTML(self.as_html(labels=labels,\n",
        "                                  predict_proba=predict_proba,\n",
        "                                  show_predicted_value=show_predicted_value,\n",
        "                                  **kwargs)))\n",
        "\n",
        "    def save_to_file(self,\n",
        "                     file_path,\n",
        "                     labels=None,\n",
        "                     predict_proba=True,\n",
        "                     show_predicted_value=True,\n",
        "                     **kwargs):\n",
        "        \"\"\"Saves html explanation to file. .\n",
        "        Params:\n",
        "            file_path: file to save explanations to\n",
        "        See as_html() for additional parameters.\n",
        "        \"\"\"\n",
        "        file_ = open(file_path, 'w', encoding='utf8')\n",
        "        file_.write(self.as_html(labels=labels,\n",
        "                                 predict_proba=predict_proba,\n",
        "                                 show_predicted_value=show_predicted_value,\n",
        "                                 **kwargs))\n",
        "        file_.close()\n",
        "\n",
        "    def as_html(self,\n",
        "                labels=None,\n",
        "                predict_proba=True,\n",
        "                show_predicted_value=True,\n",
        "                **kwargs):\n",
        "        \"\"\"Returns the explanation as an html page.\n",
        "        Args:\n",
        "            labels: desired labels to show explanations for (as barcharts).\n",
        "                If you ask for a label for which an explanation wasn't\n",
        "                computed, will throw an exception. If None, will show\n",
        "                explanations for all available labels. (only used for classification)\n",
        "            predict_proba: if true, add  barchart with prediction probabilities\n",
        "                for the top classes. (only used for classification)\n",
        "            show_predicted_value: if true, add  barchart with expected value\n",
        "                (only used for regression)\n",
        "            kwargs: keyword arguments, passed to domain_mapper\n",
        "        Returns:\n",
        "            code for an html page, including javascript includes.\n",
        "        \"\"\"\n",
        "\n",
        "        def jsonize(x):\n",
        "            return json.dumps(x, ensure_ascii=False)\n",
        "\n",
        "        if labels is None and self.mode == \"classification\":\n",
        "            labels = self.available_labels()\n",
        "\n",
        "        this_dir, _ = os.path.split(__file__)\n",
        "        bundle = open(os.path.join(this_dir, 'bundle.js'),\n",
        "                      encoding=\"utf8\").read()\n",
        "\n",
        "        out = u'''<html>\n",
        "        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF8\">\n",
        "        <head><script>%s </script></head><body>''' % bundle\n",
        "        random_id = id_generator(size=15, random_state=check_random_state(self.random_state))\n",
        "        out += u'''\n",
        "        <div class=\"lime top_div\" id=\"top_div%s\"></div>\n",
        "        ''' % random_id\n",
        "\n",
        "        predict_proba_js = ''\n",
        "        if self.mode == \"classification\" and predict_proba:\n",
        "            predict_proba_js = u'''\n",
        "            var pp_div = top_div.append('div')\n",
        "                                .classed('lime predict_proba', true);\n",
        "            var pp_svg = pp_div.append('svg').style('width', '100%%');\n",
        "            var pp = new lime.PredictProba(pp_svg, %s, %s);\n",
        "            ''' % (jsonize([str(x) for x in self.class_names]),\n",
        "                   jsonize(list(self.predict_proba.astype(float))))\n",
        "\n",
        "        predict_value_js = ''\n",
        "        if self.mode == \"regression\" and show_predicted_value:\n",
        "            # reference self.predicted_value\n",
        "            # (svg, predicted_value, min_value, max_value)\n",
        "            predict_value_js = u'''\n",
        "                    var pp_div = top_div.append('div')\n",
        "                                        .classed('lime predicted_value', true);\n",
        "                    var pp_svg = pp_div.append('svg').style('width', '100%%');\n",
        "                    var pp = new lime.PredictedValue(pp_svg, %s, %s, %s);\n",
        "                    ''' % (jsonize(float(self.predicted_value)),\n",
        "                           jsonize(float(self.min_value)),\n",
        "                           jsonize(float(self.max_value)))\n",
        "\n",
        "        exp_js = '''var exp_div;\n",
        "            var exp = new lime.Explanation(%s);\n",
        "        ''' % (jsonize([str(x) for x in self.class_names]))\n",
        "\n",
        "        if self.mode == \"classification\":\n",
        "            for label in labels:\n",
        "                exp = jsonize(self.as_list(label))\n",
        "                exp_js += u'''\n",
        "                exp_div = top_div.append('div').classed('lime explanation', true);\n",
        "                exp.show(%s, %d, exp_div);\n",
        "                ''' % (exp, label)\n",
        "        else:\n",
        "            exp = jsonize(self.as_list())\n",
        "            exp_js += u'''\n",
        "            exp_div = top_div.append('div').classed('lime explanation', true);\n",
        "            exp.show(%s, %s, exp_div);\n",
        "            ''' % (exp, self.dummy_label)\n",
        "\n",
        "        raw_js = '''var raw_div = top_div.append('div');'''\n",
        "\n",
        "        if self.mode == \"classification\":\n",
        "            html_data = self.local_exp[labels[0]]\n",
        "        else:\n",
        "            html_data = self.local_exp[self.dummy_label]\n",
        "\n",
        "        raw_js += self.domain_mapper.visualize_instance_html(\n",
        "                html_data,\n",
        "                labels[0] if self.mode == \"classification\" else self.dummy_label,\n",
        "                'raw_div',\n",
        "                'exp',\n",
        "                **kwargs)\n",
        "        out += u'''\n",
        "        <script>\n",
        "        var top_div = d3.select('#top_div%s').classed('lime top_div', true);\n",
        "        %s\n",
        "        %s\n",
        "        %s\n",
        "        %s\n",
        "        </script>\n",
        "        ''' % (random_id, predict_proba_js, predict_value_js, exp_js, raw_js)\n",
        "        out += u'</body></html>'\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3uPPjUgLzLU",
        "colab_type": "text"
      },
      "source": [
        "**lime-tabular.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAhl5i_R3C4i",
        "colab_type": "text"
      },
      "source": [
        "*lime-tabular.py* contains functions for explaining classifiers (using *lime-base.py* and *explanation.py*) that use tabular data (matrices).\n",
        "\n",
        "For more info about input parameters of LimeTabularExplainer class check the code with comments below.\n",
        "\n",
        "Since lime-base uses perturbed data, for numerical features, LimeTabularExplainer perturb them using the *data_inverse* function (explained later) before passing this perturbed data to the lime-base.py function that do the explanation.\n",
        "\n",
        "In LimeTablurar Explainer object, input parameters for the inizialization are : the training data in which are istances to be explained, the mode of explanation ( regression and classification), features names, class names and categorical features and their names. Other parameters are the kernel, that compute a measure of similarity according to euclidean distance,  feature selection method or a value (sample_around_instance ) that if true will sample continuous features in perturbed samples from a normal centered at the instance being explained. Otherwise, the normal is centered on the mean of the feature data. Finally it can be chosen of discretizing all non categorical features, chosing the discretizer type ( quartile, entropy etc). If discretizing is on, a dict object having the details of training data statistics can be specified. For more details about functions check the code below.\n",
        "\n",
        "The most impotant function of LimeTabularExplainer object is *explain_instance*, that generates explanations for a prediction. Firstly, neighborhood data are generated by randomly perturbing features from the instance. Then are learned locally weighted linear models on this neighborhood data to explain each of the classes in an interpretable way, using lime-based.py functions. The function return an Explanation object (see explanation.py) with the corresponding explanations.\n",
        "\n",
        "Function's input parameters are a data row (because explanation can be produced for a single instance),  prediction function that must be a probability function for classifier(LIME does not currently support classifier models without probability scores), labels to be explained specifying  (not mandatory) the most likely labels. Other parameters are the maximun number of features for explanation, the size of neighborhood to learn the linear model, the distance metric to use for weights and the regressor to use in explanation ( defaults to ridge regression). \n",
        "\n",
        "First of all explain_instance function calls data_inverse function that generates a neighborhood around a prediction. For numerical features, perturb them by sampling from a Normal(0,1) and\n",
        "doing the inverse operation of mean-centering and scaling, according to the means and stds in the training data. For categorical features, perturb by sampling according to the training distribution, and making a binary feature that is 1 when the value is the same as the instance being explained. The input parameters are a data row and number of samples from neighborhood ( same as explain_instance function). The function returns a tuple (data, inverse) where \n",
        "*  data - dense num_samples * K matrix, where categorical features  are encoded with either 0 (not equal to the corresponding value in data_row) or 1. The first row is the original instance.\n",
        "* inverse - same as data, except the categorical features are not binary, but categorical (as the original data)\n",
        "\n",
        "Parameters like pertubed data, num_features exc. are passed to lime-base.py to run feature selection and explanations.\n",
        "\n",
        "After computing distance, function probabilities, neighborhood data exc. (For more info look at the code below), All this parameters are passed to lime-base.py funtion *explain_instance_with_data*, to generate the explanation. Moreover an Explanation object  (from explanation.py) is created to show results with the visualization functions specified previously. \n",
        "           \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGvATDYiL5Sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import collections\n",
        "import copy\n",
        "from functools import partial\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import sklearn\n",
        "import sklearn.preprocessing\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "from lime.discretize import QuartileDiscretizer\n",
        "from lime.discretize import DecileDiscretizer\n",
        "from lime.discretize import EntropyDiscretizer\n",
        "from lime.discretize import BaseDiscretizer\n",
        "from lime.discretize import StatsDiscretizer\n",
        "\n",
        "#import of two previous modules\n",
        "from . import explanation \n",
        "from . import lime_base\n",
        "\n",
        "\n",
        "class TableDomainMapper(explanation.DomainMapper):\n",
        "    \"\"\"Maps feature ids to names, generates table views, etc\"\"\"\n",
        "\n",
        "    def __init__(self, feature_names, feature_values, scaled_row,\n",
        "                 categorical_features, discretized_feature_names=None,\n",
        "                 feature_indexes=None):\n",
        "        \"\"\"Init.\n",
        "        Args:\n",
        "            feature_names: list of feature names, in order\n",
        "            feature_values: list of strings with the values of the original row\n",
        "            scaled_row: scaled row\n",
        "            categorical_features: list of categorical features ids (ints)\n",
        "            feature_indexes: optional feature indexes used in the sparse case\n",
        "        \"\"\"\n",
        "        self.exp_feature_names = feature_names\n",
        "        self.discretized_feature_names = discretized_feature_names\n",
        "        self.feature_names = feature_names\n",
        "        self.feature_values = feature_values\n",
        "        self.feature_indexes = feature_indexes\n",
        "        self.scaled_row = scaled_row\n",
        "        if sp.sparse.issparse(scaled_row):\n",
        "            self.all_categorical = False\n",
        "        else:\n",
        "            self.all_categorical = len(categorical_features) == len(scaled_row)\n",
        "        self.categorical_features = categorical_features\n",
        "\n",
        "    def map_exp_ids(self, exp):\n",
        "        \"\"\"Maps ids to feature names.\n",
        "        Args:\n",
        "            exp: list of tuples [(id, weight), (id,weight)] (exp returned from lime-base.py)\n",
        "        Returns:\n",
        "            list of tuples (feature_name, weight)\n",
        "        \"\"\"\n",
        "        names = self.exp_feature_names\n",
        "        if self.discretized_feature_names is not None:\n",
        "            names = self.discretized_feature_names\n",
        "        return [(names[x[0]], x[1]) for x in exp]\n",
        "\n",
        "    def visualize_instance_html(self,\n",
        "                                exp,\n",
        "                                label,\n",
        "                                div_name,\n",
        "                                exp_object_name,\n",
        "                                show_table=True,\n",
        "                                show_all=False):\n",
        "        \"\"\"Shows the current example in a table format.\n",
        "        Args:\n",
        "             exp: list of tuples [(id, weight), (id,weight)] (exp returned from lime-base.py)\n",
        "             label: label id (integer)\n",
        "             div_name: name of div object to be used for rendering(in js)\n",
        "             exp_object_name: name of js explanation object\n",
        "             show_table: if False, don't show table visualization.\n",
        "             show_all: if True, show zero-weighted features in the table.\n",
        "        \"\"\"\n",
        "        if not show_table:\n",
        "            return ''\n",
        "        weights = [0] * len(self.feature_names)\n",
        "        for x in exp:\n",
        "            weights[x[0]] = x[1]\n",
        "        if self.feature_indexes is not None:\n",
        "            # Sparse case: only display the non-zero values and importances\n",
        "            fnames = [self.exp_feature_names[i] for i in self.feature_indexes]\n",
        "            fweights = [weights[i] for i in self.feature_indexes]\n",
        "            if show_all:\n",
        "                out_list = list(zip(fnames,\n",
        "                                    self.feature_values,\n",
        "                                    fweights))\n",
        "            else:\n",
        "                out_dict = dict(map(lambda x: (x[0], (x[1], x[2], x[3])),\n",
        "                                zip(self.feature_indexes,\n",
        "                                    fnames,\n",
        "                                    self.feature_values,\n",
        "                                    fweights)))\n",
        "                out_list = [out_dict.get(x[0], (str(x[0]), 0.0, 0.0)) for x in exp]\n",
        "        else:\n",
        "            out_list = list(zip(self.exp_feature_names,\n",
        "                                self.feature_values,\n",
        "                                weights))\n",
        "            if not show_all:\n",
        "                out_list = [out_list[x[0]] for x in exp]\n",
        "        ret = u'''\n",
        "            %s.show_raw_tabular(%s, %d, %s);\n",
        "        ''' % (exp_object_name, json.dumps(out_list, ensure_ascii=False), label, div_name)\n",
        "        return ret\n",
        "\n",
        "\n",
        "class LimeTabularExplainer(object):\n",
        "    \"\"\"Explains predictions on tabular (i.e. matrix) data.\n",
        "    For numerical features, perturb them by sampling from a Normal(0,1) and\n",
        "    doing the inverse operation of mean-centering and scaling, according to the\n",
        "    means and stds in the training data. For categorical features, perturb by\n",
        "    sampling according to the training distribution, and making a binary\n",
        "    feature that is 1 when the value is the same as the instance being\n",
        "    explained.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 training_data,\n",
        "                 mode=\"classification\",\n",
        "                 training_labels=None,\n",
        "                 feature_names=None,\n",
        "                 categorical_features=None,\n",
        "                 categorical_names=None,\n",
        "                 kernel_width=None,\n",
        "                 kernel=None,\n",
        "                 verbose=False,\n",
        "                 class_names=None,\n",
        "                 feature_selection='auto',\n",
        "                 discretize_continuous=True,\n",
        "                 discretizer='quartile',\n",
        "                 sample_around_instance=False,\n",
        "                 random_state=None,\n",
        "                 training_data_stats=None):\n",
        "        \"\"\"Init function.\n",
        "        Args:\n",
        "            training_data: numpy 2d array\n",
        "            mode: \"classification\" or \"regression\"\n",
        "            training_labels: labels for training data. Not required, but may be\n",
        "                used by discretizer.\n",
        "            feature_names: list of names (strings) corresponding to the columns\n",
        "                in the training data.\n",
        "            categorical_features: list of indices (ints) corresponding to the\n",
        "                categorical columns. Everything else will be considered\n",
        "                continuous. Values in these columns MUST be integers.\n",
        "            categorical_names: map from int to list of names, where\n",
        "                categorical_names[x][y] represents the name of the yth value of\n",
        "                column x.\n",
        "            kernel_width: kernel width for the exponential kernel.\n",
        "                If None, defaults to sqrt (number of columns) * 0.75\n",
        "            kernel: similarity kernel that takes euclidean distances and kernel\n",
        "                width as input and outputs weights in (0,1). If None, defaults to\n",
        "                an exponential kernel.\n",
        "            verbose: if true, print local prediction values from linear model\n",
        "            class_names: list of class names, ordered according to whatever the\n",
        "                classifier is using. If not present, class names will be '0',\n",
        "                '1', ...\n",
        "            feature_selection: feature selection method. can be\n",
        "                'forward_selection', 'lasso_path', 'none' or 'auto'.\n",
        "                See function 'explain_instance_with_data' in lime_base.py for\n",
        "                details on what each of the options does.\n",
        "            discretize_continuous: if True, all non-categorical features will\n",
        "                be discretized into quartiles.\n",
        "            discretizer: only matters if discretize_continuous is True\n",
        "                and data is not sparse. Options are 'quartile', 'decile',\n",
        "                'entropy' or a BaseDiscretizer instance.\n",
        "            sample_around_instance: if True, will sample continuous features\n",
        "                in perturbed samples from a normal centered at the instance\n",
        "                being explained. Otherwise, the normal is centered on the mean\n",
        "                of the feature data.\n",
        "            random_state: an integer or numpy.RandomState that will be used to\n",
        "                generate random numbers. If None, the random state will be\n",
        "                initialized using the internal numpy seed.\n",
        "            training_data_stats: a dict object having the details of training data\n",
        "                statistics. If None, training data information will be used, only matters\n",
        "                if discretize_continuous is True. Must have the following keys:\n",
        "                means\", \"mins\", \"maxs\", \"stds\", \"feature_values\",\n",
        "                \"feature_frequencies\"\n",
        "        \"\"\"\n",
        "        self.random_state = check_random_state(random_state)\n",
        "        self.mode = mode\n",
        "        self.categorical_names = categorical_names or {}\n",
        "        self.sample_around_instance = sample_around_instance\n",
        "        self.training_data_stats = training_data_stats\n",
        "\n",
        "        # Check and raise proper error in stats are supplied in non-descritized path\n",
        "        if self.training_data_stats:\n",
        "            self.validate_training_data_stats(self.training_data_stats)\n",
        "        if categorical_features is None:\n",
        "            categorical_features = []\n",
        "            #if no feature names are specified, then take all the feature names from training data\n",
        "        if feature_names is None:\n",
        "            feature_names = [str(i) for i in range(training_data.shape[1])]\n",
        "\n",
        "        self.categorical_features = list(categorical_features)\n",
        "        self.feature_names = list(feature_names)\n",
        "\n",
        "        self.discretizer = None\n",
        "        if discretize_continuous and not sp.sparse.issparse(training_data):\n",
        "            # Set the discretizer if training data stats are provided\n",
        "            if self.training_data_stats:\n",
        "                discretizer = StatsDiscretizer(training_data, self.categorical_features,\n",
        "                                               self.feature_names, labels=training_labels,\n",
        "                                               data_stats=self.training_data_stats)\n",
        "\n",
        "            if discretizer == 'quartile':\n",
        "                self.discretizer = QuartileDiscretizer(\n",
        "                        training_data, self.categorical_features,\n",
        "                        self.feature_names, labels=training_labels)\n",
        "            elif discretizer == 'decile':\n",
        "                self.discretizer = DecileDiscretizer(\n",
        "                        training_data, self.categorical_features,\n",
        "                        self.feature_names, labels=training_labels)\n",
        "            elif discretizer == 'entropy':\n",
        "                self.discretizer = EntropyDiscretizer(\n",
        "                        training_data, self.categorical_features,\n",
        "                        self.feature_names, labels=training_labels)\n",
        "            elif isinstance(discretizer, BaseDiscretizer):\n",
        "                self.discretizer = discretizer\n",
        "            else:\n",
        "                raise ValueError('''Discretizer must be 'quartile',''' +\n",
        "                                 ''' 'decile', 'entropy' or a''' +\n",
        "                                 ''' BaseDiscretizer instance''')\n",
        "            self.categorical_features = list(range(training_data.shape[1]))\n",
        "\n",
        "            # Get the discretized_training_data when the stats are not provided\n",
        "            if(self.training_data_stats is None): #If None, training data information will be used\n",
        "                discretized_training_data = self.discretizer.discretize(\n",
        "                    training_data)\n",
        "\n",
        "        if kernel_width is None:\n",
        "            kernel_width = np.sqrt(training_data.shape[1]) * .75\n",
        "        kernel_width = float(kernel_width)\n",
        "\n",
        "        if kernel is None:\n",
        "            def kernel(d, kernel_width):\n",
        "                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
        "\n",
        "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
        "\n",
        "        self.feature_selection = feature_selection\n",
        "        self.base = lime_base.LimeBase(kernel_fn, verbose, random_state=self.random_state)\n",
        "        self.class_names = class_names\n",
        "\n",
        "        # Though set has no role to play if training data stats are provided\n",
        "        self.scaler = None\n",
        "        self.scaler = sklearn.preprocessing.StandardScaler(with_mean=False)\n",
        "        self.scaler.fit(training_data)\n",
        "        self.feature_values = {}\n",
        "        self.feature_frequencies = {}\n",
        "\n",
        "        #check if discretizer or training_data_stats are provided and set variables differently for each case\n",
        "        for feature in self.categorical_features:\n",
        "            if training_data_stats is None:\n",
        "                if self.discretizer is not None:\n",
        "                    column = discretized_training_data[:, feature]\n",
        "                else:\n",
        "                    column = training_data[:, feature]\n",
        "\n",
        "                feature_count = collections.Counter(column)\n",
        "                values, frequencies = map(list, zip(*(sorted(feature_count.items()))))\n",
        "            else:\n",
        "                values = training_data_stats[\"feature_values\"][feature]\n",
        "                frequencies = training_data_stats[\"feature_frequencies\"][feature]\n",
        "\n",
        "            self.feature_values[feature] = values\n",
        "            self.feature_frequencies[feature] = (np.array(frequencies) /\n",
        "                                                 float(sum(frequencies)))\n",
        "            self.scaler.mean_[feature] = 0\n",
        "            self.scaler.scale_[feature] = 1\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_and_round(values):\n",
        "        return ['%.2f' % v for v in values]\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_training_data_stats(training_data_stats):\n",
        "        \"\"\"\n",
        "            Method to validate the structure of training data stats\n",
        "        \"\"\"\n",
        "        stat_keys = list(training_data_stats.keys())\n",
        "        valid_stat_keys = [\"means\", \"mins\", \"maxs\", \"stds\", \"feature_values\", \"feature_frequencies\"]\n",
        "        missing_keys = list(set(valid_stat_keys) - set(stat_keys))\n",
        "        if len(missing_keys) > 0:\n",
        "            raise Exception(\"Missing keys in training_data_stats. Details:\" % (missing_keys))\n",
        "\n",
        "    def explain_instance(self,\n",
        "                         data_row,\n",
        "                         predict_fn,\n",
        "                         labels=(1,),\n",
        "                         top_labels=None,\n",
        "                         num_features=10,\n",
        "                         num_samples=5000,\n",
        "                         distance_metric='euclidean',\n",
        "                         model_regressor=None):\n",
        "        \"\"\"Generates explanations for a prediction.\n",
        "        First, we generate neighborhood data by randomly perturbing features\n",
        "        from the instance (see __data_inverse). We then learn locally weighted\n",
        "        linear models on this neighborhood data to explain each of the classes\n",
        "        in an interpretable way (see lime_base.py).\n",
        "        Args:\n",
        "            data_row: 1d numpy array or scipy.sparse matrix, corresponding to a row\n",
        "            predict_fn: prediction function. For classifiers, this should be a\n",
        "                function that takes a numpy array and outputs prediction\n",
        "                probabilities. For regressors, this takes a numpy array and\n",
        "                returns the predictions. For ScikitClassifiers, this is\n",
        "                `classifier.predict_proba()`. For ScikitRegressors, this\n",
        "                is `regressor.predict()`. The prediction function needs to work\n",
        "                on multiple feature vectors (the vectors randomly perturbed\n",
        "                from the data_row).\n",
        "            labels: iterable with labels to be explained.\n",
        "            top_labels: if not None, ignore labels and produce explanations for\n",
        "                the K labels with highest prediction probabilities, where K is\n",
        "                this parameter.\n",
        "            num_features: maximum number of features present in explanation\n",
        "            num_samples: size of the neighborhood to learn the linear model\n",
        "            distance_metric: the distance metric to use for weights.\n",
        "            model_regressor: sklearn regressor to use in explanation. Defaults\n",
        "                to Ridge regression in LimeBase. Must have model_regressor.coef_\n",
        "                and 'sample_weight' as a parameter to model_regressor.fit()\n",
        "        Returns:\n",
        "            An Explanation object (see explanation.py) with the corresponding\n",
        "            explanations.\n",
        "        \"\"\"\n",
        "        if sp.sparse.issparse(data_row) and not sp.sparse.isspmatrix_csr(data_row):\n",
        "            # Preventative code: if sparse, convert to csr format if not in csr format already\n",
        "            data_row = data_row.tocsr()\n",
        "        #data are generated from data_inverse function\n",
        "        data, inverse = self.__data_inverse(data_row, num_samples)\n",
        "        if sp.sparse.issparse(data):\n",
        "            # Note in sparse case we don't subtract mean since data would become dense, so let's multiply fro std dev\n",
        "            scaled_data = data.multiply(self.scaler.scale_)\n",
        "            # Multiplying with csr matrix can return a coo sparse matrix\n",
        "            if not sp.sparse.isspmatrix_csr(scaled_data):\n",
        "                scaled_data = scaled_data.tocsr()\n",
        "        else:\n",
        "          #if data are not sparse we can simply scale with operation of mean-centering and scaling, \n",
        "          #centering the data around 0 and scaling with respect to the standard deviation (scaled_data represent perturbed data):\n",
        "            scaled_data = (data - self.scaler.mean_) / self.scaler.scale_\n",
        "        distances = sklearn.metrics.pairwise_distances( #distances are computed according to the distance metric specified\n",
        "                scaled_data,\n",
        "                scaled_data[0].reshape(1, -1),\n",
        "                metric=distance_metric\n",
        "        ).ravel()\n",
        "      #used to predict perturbed labels to pass to lime-base.py explain_instance_with_data function\n",
        "        yss = predict_fn(inverse)\n",
        "\n",
        "        # for classification, the model needs to provide a list of tuples - classes\n",
        "        # along with prediction probabilities\n",
        "        if self.mode == \"classification\":\n",
        "            if len(yss.shape) == 1:\n",
        "                raise NotImplementedError(\"LIME does not currently support \"\n",
        "                                          \"classifier models without probability \"\n",
        "                                          \"scores. If this conflicts with your \"\n",
        "                                          \"use case, please let us know: \"\n",
        "                                          \"https://github.com/datascienceinc/lime/issues/16\")\n",
        "            elif len(yss.shape) == 2:\n",
        "                if self.class_names is None:\n",
        "                    self.class_names = [str(x) for x in range(yss[0].shape[0])]\n",
        "                else:\n",
        "                    self.class_names = list(self.class_names)\n",
        "                if not np.allclose(yss.sum(axis=1), 1.0):\n",
        "                    warnings.warn(\"\"\"\n",
        "                    Prediction probabilties do not sum to 1, and\n",
        "                    thus does not constitute a probability space.\n",
        "                    Check that you classifier outputs probabilities\n",
        "                    (Not log probabilities, or actual class predictions).\n",
        "                    \"\"\")\n",
        "            else:\n",
        "                raise ValueError(\"Your model outputs \"\n",
        "                                 \"arrays with {} dimensions\".format(len(yss.shape)))\n",
        "\n",
        "        # for regression, the output should be a one-dimensional array of predictions\n",
        "        else:\n",
        "            try:\n",
        "                assert isinstance(yss, np.ndarray) and len(yss.shape) == 1\n",
        "            except AssertionError:\n",
        "                raise ValueError(\"Your model needs to output single-dimensional \\\n",
        "                    numpyarrays, not arrays of {} dimensions\".format(yss.shape))\n",
        "\n",
        "            predicted_value = yss[0]\n",
        "            min_y = min(yss)\n",
        "            max_y = max(yss)\n",
        "\n",
        "            # add a dimension to be compatible with downstream machinery\n",
        "            yss = yss[:, np.newaxis]\n",
        "\n",
        "        feature_names = copy.deepcopy(self.feature_names)\n",
        "        #if no feature name is specified, take all feature form the data\n",
        "        if feature_names is None:\n",
        "            feature_names = [str(x) for x in range(data_row.shape[0])]\n",
        "\n",
        "            #if is sparse convert and round to float with two decimals (convert_and_round function) only data different to 0 \n",
        "        if sp.sparse.issparse(data_row):\n",
        "            values = self.convert_and_round(data_row.data)\n",
        "            feature_indexes = data_row.indices\n",
        "        else:\n",
        "            values = self.convert_and_round(data_row)\n",
        "            feature_indexes = None\n",
        "\n",
        "        for i in self.categorical_features:\n",
        "            if self.discretizer is not None and i in self.discretizer.lambdas:\n",
        "                continue\n",
        "            name = int(data_row[i])\n",
        "            if i in self.categorical_names:\n",
        "                name = self.categorical_names[i][name]\n",
        "            feature_names[i] = '%s=%s' % (feature_names[i], name)\n",
        "            values[i] = 'True'\n",
        "        categorical_features = self.categorical_features\n",
        "\n",
        "        discretized_feature_names = None\n",
        "        if self.discretizer is not None:\n",
        "            categorical_features = range(data.shape[1])\n",
        "            discretized_instance = self.discretizer.discretize(data_row)\n",
        "            discretized_feature_names = copy.deepcopy(feature_names)\n",
        "            for f in self.discretizer.names:\n",
        "                discretized_feature_names[f] = self.discretizer.names[f][int(\n",
        "                        discretized_instance[f])]\n",
        "\n",
        "        domain_mapper = TableDomainMapper(feature_names, #mapping to tabular data\n",
        "                                          values,\n",
        "                                          scaled_data[0],\n",
        "                                          categorical_features=categorical_features,\n",
        "                                          discretized_feature_names=discretized_feature_names,\n",
        "                                          feature_indexes=feature_indexes)\n",
        "        ret_exp = explanation.Explanation(domain_mapper, #create an explanation object\n",
        "                                          mode=self.mode,\n",
        "                                          class_names=self.class_names)\n",
        "        ret_exp.scaled_data = scaled_data\n",
        "        if self.mode == \"classification\":\n",
        "            ret_exp.predict_proba = yss[0]\n",
        "            if top_labels:\n",
        "                labels = np.argsort(yss[0])[-top_labels:]\n",
        "                ret_exp.top_labels = list(labels)\n",
        "                ret_exp.top_labels.reverse()\n",
        "        else:\n",
        "            ret_exp.predicted_value = predicted_value\n",
        "            ret_exp.min_value = min_y\n",
        "            ret_exp.max_value = max_y\n",
        "            labels = [0]\n",
        "        for label in labels:\n",
        "            (ret_exp.intercept[label],\n",
        "             ret_exp.local_exp[label],\n",
        "             ret_exp.score, ret_exp.local_pred) = self.base.explain_instance_with_data( #all parameter are passed to lime-base.py function that explain the instance\n",
        "                    scaled_data,\n",
        "                    yss,\n",
        "                    distances,\n",
        "                    label,\n",
        "                    num_features,\n",
        "                    model_regressor=model_regressor,\n",
        "                    feature_selection=self.feature_selection)\n",
        "\n",
        "        if self.mode == \"regression\":\n",
        "            ret_exp.intercept[1] = ret_exp.intercept[0]\n",
        "            ret_exp.local_exp[1] = [x for x in ret_exp.local_exp[0]]\n",
        "            ret_exp.local_exp[0] = [(i, -1 * j) for i, j in ret_exp.local_exp[1]]\n",
        "\n",
        "        return ret_exp\n",
        "\n",
        "    def __data_inverse(self,\n",
        "                       data_row,\n",
        "                       num_samples):\n",
        "        \"\"\"Generates a neighborhood around a prediction.\n",
        "        For numerical features, perturb them by sampling from a Normal(0,1) and\n",
        "        doing the inverse operation of mean-centering and scaling, according to\n",
        "        the means and stds in the training data. For categorical features,\n",
        "        perturb by sampling according to the training distribution, and making\n",
        "        a binary feature that is 1 when the value is the same as the instance\n",
        "        being explained.\n",
        "        Args:\n",
        "            data_row: 1d numpy array, corresponding to a row\n",
        "            num_samples: size of the neighborhood to learn the linear model\n",
        "        Returns:\n",
        "            A tuple (data, inverse), where:\n",
        "                data: dense num_samples * K matrix, where categorical features\n",
        "                are encoded with either 0 (not equal to the corresponding value\n",
        "                in data_row) or 1. The first row is the original instance.\n",
        "                inverse: same as data, except the categorical features are not\n",
        "                binary, but categorical (as the original data)\n",
        "        \"\"\"\n",
        "        #check if sparse\n",
        "        is_sparse = sp.sparse.issparse(data_row)\n",
        "        if is_sparse:\n",
        "            num_cols = data_row.shape[1]\n",
        "            data = sp.sparse.csr_matrix((num_samples, num_cols), dtype=data_row.dtype)\n",
        "        else:\n",
        "            num_cols = data_row.shape[0]\n",
        "            data = np.zeros((num_samples, num_cols))\n",
        "        categorical_features = range(num_cols)\n",
        "        if self.discretizer is None:\n",
        "            instance_sample = data_row\n",
        "            scale = self.scaler.scale_\n",
        "            mean = self.scaler.mean_\n",
        "            if is_sparse:\n",
        "                # Perturb only the non-zero values\n",
        "                non_zero_indexes = data_row.nonzero()[1]\n",
        "                num_cols = len(non_zero_indexes)\n",
        "                instance_sample = data_row[:, non_zero_indexes]\n",
        "                scale = scale[non_zero_indexes]\n",
        "                mean = mean[non_zero_indexes]\n",
        "            data = self.random_state.normal(\n",
        "                0, 1, num_samples * num_cols).reshape(\n",
        "                num_samples, num_cols)\n",
        "            if self.sample_around_instance: #sample_around_instance is passed in the inizialization of the object\n",
        "              \"\"\"if True, will sample continuous features\n",
        "                in perturbed samples from a normal centered at the instance\n",
        "                being explained. Otherwise, the normal is centered on the mean\n",
        "                of the feature data.\"\"\"\n",
        "                data = data * scale + instance_sample\n",
        "            else:\n",
        "                data = data * scale + mean\n",
        "            if is_sparse:\n",
        "                if num_cols == 0:\n",
        "                    data = sp.sparse.csr_matrix((num_samples,\n",
        "                                                 data_row.shape[1]),\n",
        "                                                dtype=data_row.dtype)\n",
        "                else:\n",
        "                    indexes = np.tile(non_zero_indexes, num_samples)\n",
        "                    indptr = np.array(\n",
        "                        range(0, len(non_zero_indexes) * (num_samples + 1),\n",
        "                              len(non_zero_indexes)))\n",
        "                    data_1d_shape = data.shape[0] * data.shape[1]\n",
        "                    data_1d = data.reshape(data_1d_shape)\n",
        "                    data = sp.sparse.csr_matrix(\n",
        "                        (data_1d, indexes, indptr),\n",
        "                        shape=(num_samples, data_row.shape[1]))\n",
        "            categorical_features = self.categorical_features\n",
        "            first_row = data_row\n",
        "            \"\"\" discretizer (init function): only matters if discretize_continuous is True\n",
        "                and data is not sparse.\"\"\"\n",
        "        else:\n",
        "            first_row = self.discretizer.discretize(data_row)\n",
        "        data[0] = data_row.copy()\n",
        "        inverse = data.copy()\n",
        "        #in inverse data, categorical features are not binary, but categorical \n",
        "        for column in categorical_features:\n",
        "            values = self.feature_values[column]\n",
        "            freqs = self.feature_frequencies[column]\n",
        "            inverse_column = self.random_state.choice(values, size=num_samples,\n",
        "                                                      replace=True, p=freqs)\n",
        "            binary_column = np.array([1 if x == first_row[column]\n",
        "                                      else 0 for x in inverse_column])\n",
        "            binary_column[0] = 1\n",
        "            inverse_column[0] = data[0, column]\n",
        "            data[:, column] = binary_column\n",
        "            inverse[:, column] = inverse_column\n",
        "        if self.discretizer is not None:\n",
        "            inverse[1:] = self.discretizer.undiscretize(inverse[1:])\n",
        "        inverse[0] = data_row\n",
        "        return data, inverse\n",
        "\n",
        "\n",
        "class RecurrentTabularExplainer(LimeTabularExplainer):\n",
        "    \"\"\"\n",
        "    An explainer for keras-style recurrent neural networks, where the\n",
        "    input shape is (n_samples, n_timesteps, n_features). This class\n",
        "    just extends the LimeTabularExplainer class and reshapes the training\n",
        "    data and feature names such that they become something like\n",
        "    (val1_t1, val1_t2, val1_t3, ..., val2_t1, ..., valn_tn)\n",
        "    Each of the methods that take data reshape it appropriately,\n",
        "    so you can pass in the training/testing data exactly as you\n",
        "    would to the recurrent neural network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, training_data, mode=\"classification\",\n",
        "                 training_labels=None, feature_names=None,\n",
        "                 categorical_features=None, categorical_names=None,\n",
        "                 kernel_width=None, kernel=None, verbose=False, class_names=None,\n",
        "                 feature_selection='auto', discretize_continuous=True,\n",
        "                 discretizer='quartile', random_state=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            training_data: numpy 3d array with shape\n",
        "                (n_samples, n_timesteps, n_features)\n",
        "            mode: \"classification\" or \"regression\"\n",
        "            training_labels: labels for training data. Not required, but may be\n",
        "                used by discretizer.\n",
        "            feature_names: list of names (strings) corresponding to the columns\n",
        "                in the training data.\n",
        "            categorical_features: list of indices (ints) corresponding to the\n",
        "                categorical columns. Everything else will be considered\n",
        "                continuous. Values in these columns MUST be integers.\n",
        "            categorical_names: map from int to list of names, where\n",
        "                categorical_names[x][y] represents the name of the yth value of\n",
        "                column x.\n",
        "            kernel_width: kernel width for the exponential kernel.\n",
        "            If None, defaults to sqrt(number of columns) * 0.75\n",
        "            kernel: similarity kernel that takes euclidean distances and kernel\n",
        "                width as input and outputs weights in (0,1). If None, defaults to\n",
        "                an exponential kernel.\n",
        "            verbose: if true, print local prediction values from linear model\n",
        "            class_names: list of class names, ordered according to whatever the\n",
        "                classifier is using. If not present, class names will be '0',\n",
        "                '1', ...\n",
        "            feature_selection: feature selection method. can be\n",
        "                'forward_selection', 'lasso_path', 'none' or 'auto'.\n",
        "                See function 'explain_instance_with_data' in lime_base.py for\n",
        "                details on what each of the options does.\n",
        "            discretize_continuous: if True, all non-categorical features will\n",
        "                be discretized into quartiles.\n",
        "            discretizer: only matters if discretize_continuous is True. Options\n",
        "                are 'quartile', 'decile', 'entropy' or a BaseDiscretizer\n",
        "                instance.\n",
        "            random_state: an integer or numpy.RandomState that will be used to\n",
        "                generate random numbers. If None, the random state will be\n",
        "                initialized using the internal numpy seed.\n",
        "        \"\"\"\n",
        "\n",
        "        # Reshape X\n",
        "        n_samples, n_timesteps, n_features = training_data.shape\n",
        "        training_data = np.transpose(training_data, axes=(0, 2, 1)).reshape(\n",
        "                n_samples, n_timesteps * n_features)\n",
        "        self.n_timesteps = n_timesteps\n",
        "        self.n_features = n_features\n",
        "\n",
        "        # Update the feature names\n",
        "        feature_names = ['{}_t-{}'.format(n, n_timesteps - (i + 1))\n",
        "                         for n in feature_names for i in range(n_timesteps)]\n",
        "\n",
        "        # Send off the the super class to do its magic.\n",
        "        super(RecurrentTabularExplainer, self).__init__(\n",
        "                training_data,\n",
        "                mode=mode,\n",
        "                training_labels=training_labels,\n",
        "                feature_names=feature_names,\n",
        "                categorical_features=categorical_features,\n",
        "                categorical_names=categorical_names,\n",
        "                kernel_width=kernel_width,\n",
        "                kernel=kernel,\n",
        "                verbose=verbose,\n",
        "                class_names=class_names,\n",
        "                feature_selection=feature_selection,\n",
        "                discretize_continuous=discretize_continuous,\n",
        "                discretizer=discretizer,\n",
        "                random_state=random_state)\n",
        "\n",
        "    def _make_predict_proba(self, func):\n",
        "        \"\"\"\n",
        "        The predict_proba method will expect 3d arrays, but we are reshaping\n",
        "        them to 2D so that LIME works correctly. This wraps the function\n",
        "        you give in explain_instance to first reshape the data to have\n",
        "        the shape the the keras-style network expects.\n",
        "        \"\"\"\n",
        "\n",
        "        def predict_proba(X):\n",
        "            n_samples = X.shape[0]\n",
        "            new_shape = (n_samples, self.n_features, self.n_timesteps)\n",
        "            X = np.transpose(X.reshape(new_shape), axes=(0, 2, 1))\n",
        "            return func(X)\n",
        "\n",
        "        return predict_proba\n",
        "\n",
        "    def explain_instance(self, data_row, classifier_fn, labels=(1,),\n",
        "                         top_labels=None, num_features=10, num_samples=5000,\n",
        "                         distance_metric='euclidean', model_regressor=None):\n",
        "        \"\"\"Generates explanations for a prediction.\n",
        "        First, we generate neighborhood data by randomly perturbing features\n",
        "        from the instance (see __data_inverse). We then learn locally weighted\n",
        "        linear models on this neighborhood data to explain each of the classes\n",
        "        in an interpretable way (see lime_base.py).\n",
        "        Args:\n",
        "            data_row: 2d numpy array, corresponding to a row\n",
        "            classifier_fn: classifier prediction probability function, which\n",
        "                takes a numpy array and outputs prediction probabilities. For\n",
        "                ScikitClassifiers , this is classifier.predict_proba.\n",
        "            labels: iterable with labels to be explained.\n",
        "            top_labels: if not None, ignore labels and produce explanations for\n",
        "                the K labels with highest prediction probabilities, where K is\n",
        "                this parameter.\n",
        "            num_features: maximum number of features present in explanation\n",
        "            num_samples: size of the neighborhood to learn the linear model\n",
        "            distance_metric: the distance metric to use for weights.\n",
        "            model_regressor: sklearn regressor to use in explanation. Defaults\n",
        "                to Ridge regression in LimeBase. Must have\n",
        "                model_regressor.coef_ and 'sample_weight' as a parameter\n",
        "                to model_regressor.fit()\n",
        "        Returns:\n",
        "            An Explanation object (see explanation.py) with the corresponding\n",
        "            explanations.\n",
        "        \"\"\"\n",
        "\n",
        "        # Flatten input so that the normal explainer can handle it\n",
        "        data_row = data_row.T.reshape(self.n_timesteps * self.n_features)\n",
        "\n",
        "        # Wrap the classifier to reshape input\n",
        "        classifier_fn = self._make_predict_proba(classifier_fn)\n",
        "        return super(RecurrentTabularExplainer, self).explain_instance(\n",
        "            data_row, classifier_fn,\n",
        "            labels=labels,\n",
        "            top_labels=top_labels,\n",
        "            num_features=num_features,\n",
        "            num_samples=num_samples,\n",
        "            distance_metric=distance_metric,\n",
        "            model_regressor=model_regressor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwyKZD4JhOXA",
        "colab_type": "text"
      },
      "source": [
        "**submodular-pick.py**\n",
        "\n",
        "Although an explanation of a single prediction providessome understanding into the reliability of the classifier to the user, it is not sufficient to evaluate and assess trust in the model as a whole. submodular-pick.py try to give a global understanding of the model by explaining a set of individual instances. \n",
        "\n",
        "Instances need to be selected judiciously, since users may not have the time to examine a large number of explanations. We represent the time/patience that humans have by a budget B that denotes the number of explanations they are willing to look at in order to understand a model.\n",
        "Given a set of instances X, we define the pick step as the task of selecting B instances for the user to inspect.\n",
        "\n",
        "The pick step should take into account the explanations that accompany each prediction. Moreover, this method should pick a diverse, representative set of explanations to show the user\n",
        "\n",
        "The class for submodular pick saves a representative sample of explanation objects using SP-LIME, as well as saving all generated explanations First, a collection of candidate explanations are generated (see explain_instance from lime-tubular.py). From these candidates, num_exps_desired are chosen using submodular pick.\n",
        "\n",
        "The submodular pick object take as input parameters:  an array where each row is a single input into predict_fn, that is the prediction function. For classifiers, this isfunction that takes a numpy array and outputs prediction probabilities. For regressors, this returns the predictions. Other parameters are the method to use to generate candidate explanations (sample, full), the number of instances to explain if method is 'sample', the number of explanation objects returned and the maximum number of features present in explanation.\n",
        "Then two values are returned :  a list of explanation objects that has a high coverage and all the candidate explanations saved for potential future use.\n",
        "\n",
        "In particular if the method = 'sample', the rows to be explained are chosen among all the data randomly, while if the method is equal to full  then explanations will be generated for the entire data. Then the function  generate for each sample the explanations using the *explain_instance* function from lime-tabular.py \n",
        "\n",
        "To select the explanations with most converage and importance, a NxD (n instances and d features) explanation matrix W  is constructed, representing the local importante of the interpretable components for each istance. Intuitively features that explain many different instances should have higher importance scores. \n",
        "\n",
        "In the code below, once computed the dimension d (num of features in explanations of selected samples),  are created the matrix and the importance according to the formula in the paper.\n",
        "The SP-LIME greedy algorithm is run to maximize a weighted coverage function. This algorithm calculate the marginal coverage gain of adding an instance i to a set V. The instance with the highest marginal converage gain is added to the solution in each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDsETvmwhKx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "\n",
        "class SubmodularPick(object):\n",
        "   \n",
        "    def __init__(self,\n",
        "                 explainer,\n",
        "                 data,\n",
        "                 predict_fn,\n",
        "                 method='sample',\n",
        "                 sample_size=1000,\n",
        "                 num_exps_desired=5,\n",
        "                 num_features=10,\n",
        "                 **kwargs):\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: a numpy array where each row is a single input into predict_fn\n",
        "            predict_fn: prediction function. For classifiers, this should be a\n",
        "                    function that takes a numpy array and outputs prediction\n",
        "                    probabilities. For regressors, this takes a numpy array and\n",
        "                    returns the predictions. For ScikitClassifiers, this is\n",
        "                    `classifier.predict_proba()`. For ScikitRegressors, this\n",
        "                    is `regressor.predict()`. The prediction function needs to work\n",
        "                    on multiple feature vectors (the vectors randomly perturbed\n",
        "                    from the data_row).\n",
        "            method: The method to use to generate candidate explanations\n",
        "                    method == 'sample' will sample the data uniformly at\n",
        "                    random. The sample size is given by sample_size. Otherwise\n",
        "                    if method == 'full' then explanations will be generated for the\n",
        "                    entire data. l\n",
        "            sample_size: The number of instances to explain if method == 'sample'\n",
        "            num_exps_desired: The number of explanation objects returned\n",
        "            num_features: maximum number of features present in explanation\n",
        "        Sets value:\n",
        "            sp_explanations: A list of explanation objects that has a high coverage\n",
        "            explanations: All the candidate explanations saved for potential future use.\n",
        "              \"\"\"\n",
        "\n",
        "        top_labels = kwargs.get('top_labels', 1)\n",
        "        if 'top_labels' in kwargs:\n",
        "            del kwargs['top_labels']\n",
        "        # Parse args\n",
        "        #if sample the method is sample and the sample size is higher than the lenght of the rows to analyze, raise a warning and set the sample size \n",
        "        # equal to length of the data \n",
        "        if method == 'sample':\n",
        "            if sample_size > len(data):\n",
        "                warnings.warn(\"\"\"Requested sample size larger than\n",
        "                              size of input data. Using all data\"\"\")\n",
        "                sample_size = len(data)\n",
        "            all_indices = np.arange(len(data))\n",
        "            # method = simple will sample the data uniformly at random and save the indices in another variable\n",
        "            np.random.shuffle(all_indices)\n",
        "            sample_indices = all_indices[:sample_size]\n",
        "        elif method == 'full':\n",
        "          #if method == 'full' then explanations will be generated for the entire data\n",
        "            sample_indices = np.arange(len(data))\n",
        "        else:\n",
        "            raise ValueError('Method must be \\'sample\\' or \\'full\\'')\n",
        "\n",
        "        # Generate Explanations\n",
        "        self.explanations = []\n",
        "        #for each sample generate the explanations with the function in lime-tabular.py\n",
        "        for i in sample_indices:\n",
        "            self.explanations.append(\n",
        "                explainer.explain_instance(\n",
        "                    data[i], predict_fn, num_features=num_features,\n",
        "                    top_labels=top_labels,\n",
        "                    **kwargs))\n",
        "        # Error handling : check if the number of desired explanations is an integer\n",
        "        # Moreover raise a warning if the number of desired explanations is higher then all the explanation computed before (dependet from sample size)\n",
        "        # and set the value to the min between all the explanation ad the number desired.\n",
        "        try:\n",
        "            num_exps_desired = int(num_exps_desired)\n",
        "        except TypeError:\n",
        "            return(\"Requested number of explanations should be an integer\")\n",
        "        if num_exps_desired > len(self.explanations):\n",
        "            warnings.warn(\"\"\"Requested number of explanations larger than\n",
        "                           total number of explanations, returning all\n",
        "                           explanations instead.\"\"\")\n",
        "        num_exps_desired = min(num_exps_desired, len(self.explanations))\n",
        "\n",
        "        # Find all the explanation model features used. Defines the dimension d'\n",
        "        # counting the features that occurr among all explanations\n",
        "        features_dict = {}\n",
        "        feature_iter = 0\n",
        "        for exp in self.explanations:\n",
        "            labels = exp.available_labels() if exp.mode == 'classification' else [1]\n",
        "            for label in labels:\n",
        "                for feature, _ in exp.as_list(label=label):\n",
        "                    if feature not in features_dict.keys():\n",
        "                        features_dict[feature] = (feature_iter)\n",
        "                        feature_iter += 1\n",
        "        d_prime = len(features_dict.keys())\n",
        "\n",
        "        # Create the n x d' dimensional 'explanation matrix', W\n",
        "        W = np.zeros((len(self.explanations), d_prime))\n",
        "        for i, exp in enumerate(self.explanations):\n",
        "            labels = exp.available_labels() if exp.mode == 'classification' else [1]\n",
        "            for label in labels:\n",
        "                for feature, value in exp.as_list(label):\n",
        "                    W[i, features_dict[feature]] += value\n",
        "\n",
        "        # Create the global importance vector, I_j described in the paper\n",
        "        importance = np.sum(abs(W), axis=0)**.5\n",
        "\n",
        "        # Now run the SP-LIME greedy algorithm\n",
        "        remaining_indices = set(range(len(self.explanations)))\n",
        "        V = []\n",
        "        for _ in range(num_exps_desired):\n",
        "            best = 0\n",
        "            best_ind = None\n",
        "            current = 0\n",
        "            for i in remaining_indices:\n",
        "                current = np.dot( #dot product with the importance to compute the coverage\n",
        "                        (np.sum(abs(W)[V + [i]], axis=0) > 0), importance\n",
        "                        )  # coverage function \n",
        "                if current >= best:\n",
        "                    best = current\n",
        "                    best_ind = i\n",
        "            V.append(best_ind)\n",
        "            remaining_indices -= {best_ind}\n",
        "        #save selected explanations \n",
        "        self.sp_explanations = [self.explanations[i] for i in V]\n",
        "        self.V = V"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}